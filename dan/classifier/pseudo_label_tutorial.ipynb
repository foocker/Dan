{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic idear come from https://github.com/anirudhshenoy/pseudo_labeling_small_datasets/blob/master/pseudo_label-DL.ipynb\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 含义很简单\n",
    "可以从原始标注数据中分离一部分，去掉其标签，也可以使用测试数据，或者外来数据，作为生成伪标签的数据A，首先训练有监督模型，然后在数据A上预测，训练(训练伪标签的同时，按策略的训练原始标签数据)，作为最终模型。  \n",
    "需要实现：  \n",
    "1. 数据的划分，有标签的去标签，测试数据按比例   \n",
    "2. 训练策略  \n",
    "3. 渐进式划分，划分比例的自动选取  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data like mnist\n",
    "# train_loader = None  \n",
    "# unlabeled_loader = None  # split form train data\n",
    "# test_loader = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Architecture from : https://github.com/peimengsui/semi_supervised_mnist\n",
    "class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 20, kernel_size=5)\n",
    "            self.conv2 = nn.Conv2d(20, 40, kernel_size=5)\n",
    "            self.conv2_drop = nn.Dropout2d()\n",
    "            self.fc1 = nn.Linear(640, 150)\n",
    "            self.fc2 = nn.Linear(150, 10)\n",
    "            self.log_softmax = nn.LogSoftmax(dim = 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.view(-1,1,28,28)\n",
    "            x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "            x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "            x = x.view(-1, 640)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.dropout(x, training=self.training)\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.log_softmax(x)\n",
    "            return x\n",
    "        \n",
    "net = Net().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0 \n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data = data.cuda()\n",
    "            output = model(data)\n",
    "            predicted = torch.max(output,1)[1]\n",
    "            correct += (predicted == labels.cuda()).sum()\n",
    "            loss += F.nll_loss(output, labels.cuda()).item()\n",
    "\n",
    "    return (float(correct)/len(test)) *100, (loss/len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_supervised(model, train_loader, test_loader):\n",
    "    optimizer = torch.optim.SGD( model.parameters(), lr = 0.1)\n",
    "    EPOCHS = 100\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        correct = 0\n",
    "        running_loss = 0\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            X_batch, y_batch = X_batch.cuda(), y_batch.cuda()\n",
    "            \n",
    "            output = model(X_batch)\n",
    "            labeled_loss = F.nll_loss(output, y_batch)\n",
    "                       \n",
    "            optimizer.zero_grad()\n",
    "            labeled_loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += labeled_loss.item()\n",
    "        \n",
    "        if epoch %10 == 0:\n",
    "            test_acc, test_loss = evaluate(model, test_loader)\n",
    "            print('Epoch: {} : Train Loss : {:.5f} | Test Acc : {:.5f} | Test Loss : {:.3f} '.format(epoch, running_loss/(10 * len(train)), test_acc, test_loss))\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_supervised(net, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1 = 100\n",
    "T2 = 700\n",
    "af = 3\n",
    "\n",
    "def alpha_weight(epoch):\n",
    "    if epoch < T1:\n",
    "        return 0.0\n",
    "    elif epoch > T2:\n",
    "        return af\n",
    "    else:\n",
    "         return ((epoch-T1) / (T2-T1))*af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_acc, test_loss = evaluate(net, test_loader)\n",
    "# print('Test Acc : {:.5f} | Test Loss : {:.3f} '.format(test_acc, test_loss))\n",
    "# torch.save(net.state_dict(), 'saved_models/supervised_weights')\n",
    "\n",
    "# net.load_state_dict(torch.load('saved_models/supervised_weights'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_scores = []\n",
    "unlabel = []\n",
    "pseudo_label = []\n",
    "\n",
    "alpha_log = []\n",
    "test_acc_log = []\n",
    "test_loss_log = []\n",
    "def semisup_train(model, train_loader, unlabeled_loader, test_loader):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)\n",
    "    EPOCHS = 150\n",
    "    \n",
    "    # Instead of using current epoch we use a \"step\" variable to calculate alpha_weight\n",
    "    # This helps the model converge faster\n",
    "    step = 100 \n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        for batch_idx, x_unlabeled in enumerate(unlabeled_loader):\n",
    "            \n",
    "            \n",
    "            # Forward Pass to get the pseudo labels\n",
    "            x_unlabeled = x_unlabeled[0].cuda()\n",
    "            model.eval()\n",
    "            output_unlabeled = model(x_unlabeled)\n",
    "            _, pseudo_labeled = torch.max(output_unlabeled, 1)\n",
    "            model.train()\n",
    "            \n",
    "            \n",
    "            \"\"\" ONLY FOR VISUALIZATION\"\"\"\n",
    "            if (batch_idx < 3) and (epoch % 10 == 0):\n",
    "                unlabel.append(x_unlabeled.cpu())\n",
    "                pseudo_label.append(pseudo_labeled.cpu())\n",
    "            \"\"\" ********************** \"\"\"\n",
    "            \n",
    "            # Now calculate the unlabeled loss using the pseudo label\n",
    "            output = model(x_unlabeled)\n",
    "            unlabeled_loss = alpha_weight(step) * F.nll_loss(output, pseudo_labeled)   \n",
    "            \n",
    "            # Backpropogate\n",
    "            optimizer.zero_grad()\n",
    "            unlabeled_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            # For every 50 batches train one epoch on labeled data \n",
    "            if batch_idx % 50 == 0:\n",
    "                \n",
    "                # Normal training procedure\n",
    "                for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "                    X_batch = X_batch.cuda()\n",
    "                    y_batch = y_batch.cuda()\n",
    "                    output = model(X_batch)\n",
    "                    labeled_loss = F.nll_loss(output, y_batch)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    labeled_loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                # Now we increment step by 1\n",
    "                step += 1\n",
    "                \n",
    "\n",
    "        test_acc, test_loss =evaluate(model, test_loader)\n",
    "        print('Epoch: {} : Alpha Weight : {:.5f} | Test Acc : {:.5f} | Test Loss : {:.3f} '.format(epoch, alpha_weight(step), test_acc, test_loss))\n",
    "        \n",
    "        \"\"\" LOGGING VALUES \"\"\"\n",
    "        alpha_log.append(alpha_weight(step))\n",
    "        test_acc_log.append(test_acc/100)\n",
    "        test_loss_log.append(test_loss)\n",
    "        \"\"\" ************** \"\"\"\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semisup_train(net, train_loader, unlabeled_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_acc, test_loss = evaluate(net, test_loader)\n",
    "# print('Test Acc : {:.5f} | Test Loss : {:.3f} '.format(test_acc, test_loss))\n",
    "# torch.save(net.state_dict(), 'saved_models/semi_supervised_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1605254566453",
   "display_name": "Python 3.6.10 64-bit ('CenterNet': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}